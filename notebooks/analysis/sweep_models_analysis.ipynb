{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAIN WHAT THIS NOTEBOOK IS FOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import linregress\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import dotenv\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from utils.plotting_utils import *\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42  # For PDF: embed text as text, not paths\n",
    "plt.rcParams['svg.fonttype'] = 'none'  # For SVG: embed text as text, not paths\n",
    "#plt.rcParams['font.family'] = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dir_overall = \"/scratch/groups/emmalu/seq2loc/experiments/sweep_analysis\"\n",
    "dir_runs = \"/scratch/groups/emmalu/seq2loc/sweep_experiments\"\n",
    "\n",
    "test_level1 = pd.read_csv(f\"{dir_overall}/test/test_metrics_level1/overall_metrics.csv\")\n",
    "test_level2 = pd.read_csv(f\"{dir_overall}/test/test_metrics_level2/overall_metrics.csv\")\n",
    "test_level3 = pd.read_csv(f\"{dir_overall}/test/test_metrics_level3/overall_metrics.csv\")\n",
    "\n",
    "\n",
    "valid_level1 = pd.read_csv(f\"{dir_overall}/test/test_metrics_level1/overall_metrics.csv\")\n",
    "valid_level2 = pd.read_csv(f\"{dir_overall}/test/test_metrics_level2/overall_metrics.csv\")\n",
    "valid_level3 = pd.read_csv(f\"{dir_overall}/test/test_metrics_level3/overall_metrics.csv\")\n",
    "\n",
    "avg_metrics = pd.concat([test_level1, test_level2, test_level3])\n",
    "avg_metrics = avg_metrics[avg_metrics.agg_method != \"TransformerPool\"].reset_index(drop=True)\n",
    "\n",
    "avg_metrics.to_csv(\"avg_metrics_ankit.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET NECESSARY .ENV VARIABLES\n",
    "dotenv.load_dotenv('../../.env')\n",
    "SWEEP_EXP_DIR=os.getenv(\"SWEEP_EXP_DIR\")\n",
    "SWEEP_ANALYSIS_DIR=os.getenv(\"SWEEP_ANALYSIS_DIR\")\n",
    "FIG_DIR = \"../../figures/sweep_models\"\n",
    "\n",
    "mappings = load_config(\"../../datasets/final/hierarchical_label_set.yaml\")\n",
    "ordered_labels_level1 = mappings[\"level1\"][:-1] #get rid of plastid\n",
    "ordered_labels_level2 = mappings[\"level2\"][:-1] #get rid of plastid\n",
    "ordered_labels_level3 = mappings[\"level3\"][:-1] #get rid of plastid\n",
    "orders = [ordered_labels_level1, ordered_labels_level2, ordered_labels_level3]\n",
    "\n",
    "\n",
    "models = [\"ESM2\", \"ESM3\", \"ProtT5\", \"ProtBert\"]\n",
    "model_palette = sns.color_palette(\"plasma\", len(models))\n",
    "model_colors = {model: model_palette[i] for i, model in enumerate(models)}\n",
    "\n",
    "avg_metrics = pd.read_csv(f\"{SWEEP_ANALYSIS_DIR}/overall_metrics.csv\")\n",
    "\n",
    "hou_testset = pd.read_csv(\"../..datasets/final/hou_testset.csv\")\n",
    "hpa_uniprot_combined_trainset = pd.read_csv(\"../..datasets/final/hpa_uniprot_combined_trainset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP DUPLICATE RUNS\n",
    "idx = avg_metrics.groupby(\n",
    "    [\"exp_name\",\n",
    "    \"category_level\",\n",
    "    \"metadata_file\",\n",
    "    \"clip_len\",\n",
    "    \"agg_method\",\n",
    "    \"mlp_dropout\",\n",
    "    \"loss\"\n",
    "    ])[\"macro_ap\"].idxmax()\n",
    "avg_metrics = avg_metrics.loc[idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK IF ANY RUNS ARE MISSING\n",
    "exp_names = [\"ESM2\", \"ESM3\", \"ProtT5\", \"ProtBert\"]\n",
    "category_levels = [\"level1\", \"level2\", \"level3\"]\n",
    "metadata_files = [\"hpa_trainset\", \"uniprot_trainset\", \"hpa_uniprot_combined_trainset\", \"hpa_uniprot_combined_human_trainset\"]\n",
    "clip_lens = [512, 1024, 2048]\n",
    "agg_methods = [\"MeanPool\", \"MaxPool\", \"LightAttentionPool\", \"MultiHeadAttentionPool\"]\n",
    "losses = [\"BCEWithLogitsLoss\", \"SigmoidFocalLoss\"]\n",
    "mlp_dropouts = [0, 0.25, 0.5]\n",
    "\n",
    "combs = [\n",
    "    (i,j,k,l,m,n,p) \n",
    "    for i in exp_names \n",
    "    for j in category_levels \n",
    "    for k in metadata_files \n",
    "    for l in clip_lens \n",
    "    for m in agg_methods \n",
    "    for n in losses\n",
    "    for p in mlp_dropouts]\n",
    "\n",
    "for comb in combs:\n",
    "    i,j,k,l,m,n,p = comb\n",
    "    temp = avg_metrics[\n",
    "        (avg_metrics.exp_name == i) & \n",
    "        (avg_metrics.category_level == j) & \n",
    "        (avg_metrics.metadata_file == k) & \n",
    "        (avg_metrics.clip_len == l) & \n",
    "        (avg_metrics.agg_method == m) & \n",
    "        (avg_metrics.loss == n) &\n",
    "        (avg_metrics.mlp_dropout == p)\n",
    "    ]\n",
    "    if len(temp)==0:\n",
    "        print(f\"Missing {comb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARING TRAININGSETS\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "legend_added = False\n",
    "\n",
    "custom_palette = {\n",
    "    'hpa_trainset': 'red',\n",
    "    'uniprot_trainset': 'blue',\n",
    "    'hpa_uniprot_combined_trainset': 'green',\n",
    "    'hpa_uniprot_combined_human_trainset': 'orange'\n",
    "}\n",
    "\n",
    "for j in range(2):\n",
    "    for i in [1, 2, 3]:\n",
    "        if j == 0:\n",
    "            g = sns.scatterplot(\n",
    "                data=avg_metrics[avg_metrics.category_level==f\"level{i}\"],\n",
    "                x=\"macro_ap\",\n",
    "                y=\"micro_ap\",\n",
    "                hue=\"metadata_file\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=not legend_added,  # Add legend only once\n",
    "                alpha=0.6,\n",
    "                palette=custom_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Macro AP\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro AP\")\n",
    "            if not legend_added:\n",
    "                handles, labels = axes[j][i-1].get_legend_handles_labels()\n",
    "                legend_added = True  # Set to True after adding the legend\n",
    "        elif j == 1:\n",
    "            sns.scatterplot(\n",
    "                data=avg_metrics[avg_metrics.category_level==f\"level{i}\"],\n",
    "                x=\"f1_macro\",\n",
    "                y=\"f1_micro\",\n",
    "                hue=\"metadata_file\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=False,\n",
    "                alpha=0.6,\n",
    "                palette=custom_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Macro F1\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro F1\")\n",
    "        if j == 0:\n",
    "            axes[j][i-1].set_title(f\"Level {i}\")\n",
    "\n",
    "# Remove individual legends from subplots\n",
    "for ax in axes.flat:\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Create a single legend for the entire figure, placed outside the figure\n",
    "labels = [\n",
    "    \"Training Set\",\n",
    "    \"HPA trainset\", \n",
    "    \"HPA UniProt Combined (human) trainset\", \n",
    "    \"HPA Uniport Combined trainset\", \n",
    "    \"UniProt Trainset\",\n",
    "    \"Aggregation Method\", \n",
    "    \"Light Attenion\",\n",
    "    \"Max Pool\",\n",
    "    \"Mean Pool\", \n",
    "    \"Multihead Attention\"\n",
    "    ]\n",
    "\n",
    "fig.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.01, 0.9), fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/trainset.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARING CLIP LENGHTS\n",
    "# --> No clear advantage of longer or shorter clip lengths\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "legend_added = False\n",
    "custom_palette = {\n",
    "    512: 'red',\n",
    "    1024: 'blue',\n",
    "    2048: 'green'\n",
    "}\n",
    "\n",
    "for j in range(2):\n",
    "    for i in [1, 2, 3]:\n",
    "        if j == 0:\n",
    "            g = sns.scatterplot(\n",
    "                data=avg_metrics[\n",
    "                (avg_metrics.category_level==f\"level{i}\") &\n",
    "                (avg_metrics.metadata_file==\"hpa_uniprot_combined_trainset\")\n",
    "                ],\n",
    "                x=\"macro_ap\",\n",
    "                y=\"micro_ap\",\n",
    "                hue=\"clip_len\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=not legend_added,  # Add legend only once\n",
    "                alpha=0.8,\n",
    "                palette=custom_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Macro AP\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro AP\")\n",
    "            if not legend_added:\n",
    "                handles, labels = axes[j][i-1].get_legend_handles_labels()\n",
    "                legend_added = True  # Set to True after adding the legend\n",
    "        elif j == 1:\n",
    "            sns.scatterplot(\n",
    "                data=avg_metrics[\n",
    "                (avg_metrics.category_level==f\"level{i}\") &\n",
    "                (avg_metrics.metadata_file==\"uniprot_trainset\")\n",
    "                ],\n",
    "                x=\"f1_macro\",\n",
    "                y=\"f1_micro\",\n",
    "                hue=\"clip_len\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=False,\n",
    "                alpha=0.8,\n",
    "                palette=custom_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Macro F1\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro F1\")\n",
    "        if j == 0:\n",
    "            axes[j][i-1].set_title(f\"Level {i}\")\n",
    "\n",
    "# Remove individual legends from subplots\n",
    "for ax in axes.flat:\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Create a single legend for the entire figure, placed outside the figure\n",
    "labels = [\n",
    "    \"Clip Length\",\n",
    "    \"512\", \n",
    "    \"1024\", \n",
    "    \"2048\",\n",
    "    \"Aggregation Method\", \n",
    "    \"Light Attenion\",\n",
    "    \"Max Pool\",\n",
    "    \"Mean Pool\", \n",
    "    \"Multihead Attention\"\n",
    "    ]\n",
    "\n",
    "fig.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.01, 0.9), fontsize=9)\n",
    "\n",
    "plt.savefig(f\"{FIG_DIR}/clip_len_compare.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARING LOSS FUNCTIONS\n",
    "#No clear advantage over using one loss than the other\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "legend_added = False\n",
    "custom_palette = {\n",
    "    'BCEWithLogitsLoss': 'red',\n",
    "    'SigmoidFocalLoss': 'blue'\n",
    "}\n",
    "\n",
    "for j in range(2):\n",
    "    for i in [1, 2, 3]:\n",
    "        if j == 0:\n",
    "            g = sns.scatterplot(\n",
    "                data=avg_metrics[\n",
    "                (avg_metrics.category_level==f\"level{i}\") &\n",
    "                (avg_metrics.metadata_file==\"hpa_uniprot_combined_trainset\")],\n",
    "                x=\"macro_ap\",\n",
    "                y=\"micro_ap\",\n",
    "                hue=\"loss\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=not legend_added,  # Add legend only once\n",
    "                alpha=0.8,\n",
    "                palette=custom_palette\n",
    "            )\n",
    "            if not legend_added:\n",
    "                handles, labels = axes[j][i-1].get_legend_handles_labels()\n",
    "                legend_added = True  # Set to True after adding the legend\n",
    "\n",
    "            axes[j][i-1].set_xlabel(\"Macro AP\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro AP\")\n",
    "        elif j == 1:\n",
    "            sns.scatterplot(\n",
    "                data=avg_metrics[\n",
    "                (avg_metrics.category_level==f\"level{i}\") &\n",
    "                (avg_metrics.metadata_file==\"hpa_uniprot_combined_trainset\")],\n",
    "                x=\"f1_macro\",\n",
    "                y=\"f1_micro\",\n",
    "                hue=\"loss\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=False,\n",
    "                alpha=0.8,\n",
    "                palette=custom_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Macro F1\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro F1\")\n",
    "        if j == 0:\n",
    "            axes[j][i-1].set_title(f\"Level {i}\")\n",
    "\n",
    "# Remove individual legends from subplots\n",
    "for ax in axes.flat:\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Create a single legend for the entire figure, placed outside the figure\n",
    "\n",
    "labels = [\n",
    "    \"Loss Function\",\n",
    "    \"BCEWithLogitsLoss\", \n",
    "    \"SigmoidFocalLoss\",\n",
    "    \"Aggregation Method\", \n",
    "    \"Light Attenion\",\n",
    "    \"Max Pool\",\n",
    "    \"Mean Pool\", \n",
    "    \"Multihead Attention\"\n",
    "    ]\n",
    "\n",
    "fig.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.01, 0.9), fontsize=9)\n",
    "plt.savefig(f\"{FIG_DIR}/loss_compare.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARING LOSS FUNCTIONS\n",
    "\n",
    "# Filter data for hpa_uniprot_combined_trainset\n",
    "data_combined = avg_metrics[avg_metrics.metadata_file == \"hpa_uniprot_combined_trainset\"]\n",
    "\n",
    "# Pivot the data to have loss types as columns\n",
    "pivoted = data_combined.pivot_table(\n",
    "    index=[\"exp_name\", \"category_level\", \"clip_len\", \"agg_method\"],\n",
    "    columns=\"loss\",\n",
    "    values=[\"macro_ap\", \"micro_ap\", \"f1_macro\", \"f1_micro\"]\n",
    ")\n",
    "\n",
    "diff = pivoted.xs(\"SigmoidFocalLoss\", level=1, axis=1) - pivoted.xs(\"BCEWithLogitsLoss\", level=1, axis=1)\n",
    "diff = diff.reset_index()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "legend_added = False\n",
    "\n",
    "for j in range(2):\n",
    "    for i in [1, 2, 3]:\n",
    "        if j == 0:\n",
    "            g = sns.scatterplot(\n",
    "                data=diff[\n",
    "                (diff.category_level==f\"level{i}\")],\n",
    "                x=\"macro_ap\",\n",
    "                y=\"micro_ap\",\n",
    "                hue=\"exp_name\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=not legend_added,  # Add legend only once\n",
    "                alpha=0.8,\n",
    "                s=100\n",
    "            )\n",
    "            if not legend_added:\n",
    "                handles, labels = axes[j][i-1].get_legend_handles_labels()\n",
    "                legend_added = True  # Set to True after adding the legend\n",
    "\n",
    "            axes[j][i-1].set_xlabel(\"Macro AP difference\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro AP difference\")\n",
    "\n",
    "        elif j == 1:\n",
    "            sns.scatterplot(\n",
    "                data=diff[\n",
    "                (diff.category_level==f\"level{i}\")],\n",
    "                x=\"f1_macro\",\n",
    "                y=\"f1_micro\",\n",
    "                hue=\"exp_name\",\n",
    "                style=\"agg_method\",\n",
    "                ax=axes[j][i-1],\n",
    "                legend=False,\n",
    "                alpha=0.8,\n",
    "                s=100\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Macro F1 difference\")\n",
    "            axes[j][i-1].set_ylabel(\"Micro F1 difference\")\n",
    "        if j == 0:\n",
    "            axes[j][i-1].set_title(f\"Level {i}\")\n",
    "\n",
    "        # Add axes at y=0 and x=0 lines\n",
    "        axes[j][i-1].axhline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "        axes[j][i-1].axvline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Remove individual legends from subplots\n",
    "for ax in axes.flat:\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# Create a single legend for the entire figure, placed outside the figure\n",
    "\n",
    "labels = [\n",
    "    \"Protein Language Model\",\n",
    "    \"ESM2\", \n",
    "    \"ESM3\", \n",
    "    \"ProtBert\", \n",
    "    \"ProtT5\",\n",
    "    \"Aggregation Method\", \n",
    "    \"Light Attenion\",\n",
    "    \"Max Pool\",\n",
    "    \"Mean Pool\", \n",
    "    \"Multihead Attention\"\n",
    "    ]\n",
    "\n",
    "fig.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.01, 0.9), fontsize=12)\n",
    "fig.suptitle(\"Improvement using SigmoidFocalLoss over BCEWithLogitsLoss\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/loss_diff_compare.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARE ALL PLM x AGGREGATION STRATEGIES\n",
    "# Only looking at models trained on uniprot_trainset\n",
    "data_uniprot_combined_trainset = avg_metrics[\n",
    "    (avg_metrics.metadata_file == \"hpa_uniprot_combined_trainset\")\n",
    "]\n",
    "\n",
    "# Get models with hyperparams that resulted in best macro_ap\n",
    "idx = data_uniprot_combined_trainset.groupby(\n",
    "    [\"exp_name\", \"agg_method\", \"category_level\"]\n",
    ").macro_ap.transform(max) == data_uniprot_combined_trainset['macro_ap']\n",
    "temp = data_uniprot_combined_trainset[idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "legend_added = False\n",
    "for j in range(2):\n",
    "    for i in [1, 2, 3]:\n",
    "        if j == 0:\n",
    "            g = sns.scatterplot(\n",
    "                data=temp[temp.category_level == f\"level{i}\"],\n",
    "                x=\"micro_ap\",\n",
    "                y=\"macro_ap\",\n",
    "                hue=\"exp_name\",\n",
    "                hue_order=[\"ProtBert\", \"ProtT5\", \"ESM2\", \"ESM3\"],\n",
    "                style=\"agg_method\",\n",
    "                style_order=[\"MaxPool\", \"MeanPool\", \"LightAttentionPool\", \"MultiHeadAttentionPool\"],\n",
    "                ax=axes[j][i-1],\n",
    "                legend=not legend_added,  # Add legend only once\n",
    "                alpha=0.8,\n",
    "                s=200,\n",
    "                palette=model_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Micro AP\")\n",
    "            axes[j][i-1].set_ylabel(\"Macro AP\")\n",
    "            axes[j][i-1].set_ylim()\n",
    "            if not legend_added:\n",
    "                handles, labels = axes[j][i-1].get_legend_handles_labels()\n",
    "                legend_added = True  # Set to True after adding the legend\n",
    "        elif j == 1:\n",
    "            sns.scatterplot(\n",
    "                data=temp[temp.category_level == f\"level{i}\"],\n",
    "                x=\"f1_micro\",\n",
    "                y=\"f1_macro\",\n",
    "                hue=\"exp_name\",\n",
    "                hue_order=[\"ProtBert\", \"ProtT5\", \"ESM2\", \"ESM3\"],\n",
    "                style=\"agg_method\",\n",
    "                style_order=[\"MaxPool\", \"MeanPool\", \"LightAttentionPool\", \"MultiHeadAttentionPool\"],\n",
    "                ax=axes[j][i-1],\n",
    "                legend=False,\n",
    "                alpha=0.8,\n",
    "                s=200,\n",
    "                palette=model_palette\n",
    "            )\n",
    "            axes[j][i-1].set_xlabel(\"Micro F1\")\n",
    "            axes[j][i-1].set_ylabel(\"Macro F1\")\n",
    "        if j == 0:\n",
    "            axes[j][i-1].set_title(f\"Level {i}\")\n",
    "\n",
    "# Create a single legend for the entire figure, placed outside the figure\n",
    "custom_labels = [\n",
    " \"Protein Language Model\",\n",
    " 'ProtBert',\n",
    " 'ProtT5',\n",
    " 'ESM2',\n",
    " 'ESM3',\n",
    " 'Aggregation Method',\n",
    " 'Max',\n",
    " 'Mean',\n",
    " 'LightAttention',\n",
    " 'MultiHeadAttention']\n",
    "\n",
    "legend = fig.legend(\n",
    "    handles, \n",
    "    custom_labels, \n",
    "    loc='upper left', \n",
    "    bbox_to_anchor=(1.01, 0.9), \n",
    "    fontsize=9, \n",
    "    #facecolor='white', \n",
    "    #edgecolor='white',\n",
    "    title=None)\n",
    "\n",
    "# Remove individual legends from subplots\n",
    "for ax in axes.flat:\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{FIG_DIR}/PLMxAGG_compare.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARE ALL PLM x AGGREGATION STRATEGIES - Level 1 only\n",
    "data_uniprot_combined_trainset = avg_metrics[\n",
    "    (avg_metrics.metadata_file == \"hpa_uniprot_combined_trainset\")\n",
    "]\n",
    "\n",
    "# Get models with hyperparams that resulted in best macro_ap\n",
    "idx = data_uniprot_combined_trainset.groupby(\n",
    "    [\"exp_name\", \"agg_method\", \"category_level\"]\n",
    ").macro_ap.transform(max) == data_uniprot_combined_trainset['macro_ap']\n",
    "temp = data_uniprot_combined_trainset[idx]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(4.5, 4))\n",
    "legend_added = False\n",
    "g = sns.scatterplot(\n",
    "    data=temp[temp.category_level == f\"level1\"],\n",
    "    x=\"micro_ap\",\n",
    "    y=\"macro_ap\",\n",
    "    hue=\"exp_name\",\n",
    "    hue_order=[\"ProtBert\", \"ProtT5\", \"ESM2\", \"ESM3\"],\n",
    "    style=\"agg_method\",\n",
    "    style_order=[\"MaxPool\", \"MeanPool\", \"LightAttentionPool\", \"MultiHeadAttentionPool\"],\n",
    "    ax=axes,\n",
    "    legend=False,  # Add legend only once\n",
    "    alpha=0.8,\n",
    "    s=200,\n",
    "    palette=model_palette\n",
    ")\n",
    "axes.set_xlabel(\"Micro AP\")\n",
    "axes.set_ylabel(\"Macro AP\")\n",
    "axes.set_ylim()\n",
    "           \n",
    "# Create a single legend for the entire figure, placed outside the figure\n",
    "custom_labels = [\n",
    " \"Protein Language Model\",\n",
    " 'ProtBert',\n",
    " 'ProtT5',\n",
    " 'ESM2',\n",
    " 'ESM3',\n",
    " 'Aggregation Method',\n",
    " 'Max',\n",
    " 'Mean',\n",
    " 'LightAttention',\n",
    " 'MultiHeadAttention']\n",
    "\n",
    "legend = fig.legend(\n",
    "    handles, \n",
    "    custom_labels, \n",
    "    loc='upper left', \n",
    "    bbox_to_anchor=(1.01, 0.9), \n",
    "    fontsize=9, \n",
    "    #facecolor='white', \n",
    "    #edgecolor='white',\n",
    "    title=None)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/PLMxAGG_macroap_level1.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save average and perclass metrics for best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display best results for each PLM to add to table in manuscript\n",
    "\n",
    "#Only look at models trains on combined trainset\n",
    "data_uniprot_combined_trainset = avg_metrics[\n",
    "    (avg_metrics.metadata_file == \"hpa_uniprot_combined_trainset\")\n",
    "]\n",
    "\n",
    "# Get models with hyperparams that resulted in best macro_ap\n",
    "idx = data_uniprot_combined_trainset.groupby(\n",
    "    [\"exp_name\", \"category_level\"]\n",
    ").macro_ap.transform(max) == data_uniprot_combined_trainset['macro_ap']\n",
    "temp = data_uniprot_combined_trainset[idx]\n",
    "\n",
    "#----- SAVE PERCLASS METRICS ------\n",
    "dfs = []\n",
    "for i, row in temp.iterrows():\n",
    "    run_id = row.run_id\n",
    "    model = row.exp_name\n",
    "    metadata = row.metadata_file\n",
    "    path = f\"{d}/{model}_{metadata}/{run_id}/all_folds_perclass_metrics.csv\"\n",
    "\n",
    "    df = pd.read_csv(path).round(3)\n",
    "    df[\"Model\"] = model\n",
    "    df[\"Agg. Method\"] = row.agg_method\n",
    "    df[\"Level\"] = row.category_level[-1]\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.concat(dfs).to_csv(f\"{FIG_DIR}/cutom_models_perclass_metrics.csv\")\n",
    "\n",
    "\n",
    "#----- SAVE AVG METRICS ------\n",
    "values = [\"acc_samples\", \"macro_ap\", \"micro_ap\", \n",
    "            \"f1_macro\", \"f1_micro\", \"num_labels\",\n",
    "            \"jaccard_macro\", \"jaccard_micro\", \"rocauc_macro\",\n",
    "            \"rocauc_micro\",\t\"mlrap\", \"coverage_error\"\n",
    "        ]\n",
    "temp_for_display = temp.pivot_table(index='exp_name', columns='category_level', values=values).round(3)\n",
    "temp_for_display = temp_for_display.reindex([\"ESM2\", \"ESM3\", \"ProtT5\", \"ProtBert\"])\n",
    "display(temp_for_display)\n",
    "temp_for_display.to_csv(f\"{FIG_DIR}/custom_models_avg_metrics.csv\") #THIS FILE DOESNT ACTUALLY EXIST NEED TO MAKE IT FROM WHAT I SAVED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at which hyperparameters resulted in the best model performance for each PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only looking at models trained on hpa_uniprot_combined_trainset\n",
    "data_uniprot_combined_trainset= avg_metrics[\n",
    "    (avg_metrics.metadata_file==\"hpa_uniprot_combined_trainset\")\n",
    "    ]\n",
    "\n",
    "#Get models with hyperparams that resulted in best macro_ap\n",
    "idx = data_uniprot_combined_trainset.groupby(\n",
    "    [\"exp_name\", \"agg_method\", \"category_level\"]\n",
    "    ).macro_ap.transform(max) == data_uniprot_combined_trainset['macro_ap']\n",
    "temp = data_uniprot_combined_trainset[idx]\n",
    "\n",
    "#Sort agg_methods\n",
    "ordering = [\"MaxPool\", \"MeanPool\", \"LightAttentionPool\", \"MultiHeadAttentionPool\"]\n",
    "agg_method_dtype = pd.CategoricalDtype(categories=ordering, ordered=True)\n",
    "temp['agg_method'] = temp['agg_method'].astype(agg_method_dtype)\n",
    "temp = temp.sort_values('agg_method')\n",
    "\n",
    "temp = temp.pivot(\n",
    "    index=[\"category_level\", \"agg_method\"], \n",
    "    columns=\"exp_name\", \n",
    "    values=[\"clip_len\", \"mlp_dropout\", \"acc_samples\", \"macro_ap\", \"micro_ap\", \"f1_macro\", \"f1_micro\"])\n",
    "\n",
    "display(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision/recall bar plot per-class. One bar for every plm. Only use results for models trained on hpa_uniprot combined trainset and using best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = avg_metrics[\n",
    "    (avg_metrics.metadata_file==\"hpa_uniprot_combined_trainset\")\n",
    "].groupby([\"exp_name\", \"category_level\"])[\"macro_ap\"].idxmax().values\n",
    "\n",
    "perclass_metrics = []\n",
    "for i, row in avg_metrics.iloc[idx].iterrows():\n",
    "    path = f\"{SWEEP_EXP_DIR}/{row.exp_name}_{row.metadata_file}/{row.run_id}/all_folds_perclass_metrics.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"exp_name\"] = row.exp_name\n",
    "    df[\"category_level\"] = row.category_level\n",
    "    df[\"metadata_file\"] = row.metadata_file\n",
    "    df[\"clip_len\"] = row.clip_len\n",
    "    df[\"agg_method\"] = row.agg_method\n",
    "    df[\"loss\"] = row.loss\n",
    "    perclass_metrics.append(df)\n",
    "\n",
    "perclass_metrics = pd.concat(perclass_metrics)\n",
    "\n",
    "perclass_metrics = perclass_metrics.rename({\"category\": \"label\", \"exp_name\": \"model\"}, axis=1)\n",
    "for level in [1,2,3]:\n",
    "    temp = perclass_metrics[perclass_metrics.category_level == f\"level{level}\"]\n",
    "    fig, ax = plot_perclas_double_bar(\n",
    "        temp,\n",
    "        {\"Recall\": \"recall\",\n",
    "        \"Precision\": \"precision\"},\n",
    "        models,\n",
    "        model_colors,\n",
    "        orders[level-1]\n",
    "    )\n",
    "    plt.savefig(f\"{FIG_DIR}/custom_models_pr_barplot_level{level}.pdf\", bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_values_df = []\n",
    "\n",
    "metrics = [\"mcc\", \"acc\", \"recall\", \"precision\", \"f1\", \"jaccard\", \"rocauc\"]\n",
    "metric_title = {metrics:title for metrics, title in zip(metrics, [\"MCC\", \"ACC\", \"Recall\", \"Precision\", \"F1\", \"Jaccard\", \"ROC-AUC\"])}\n",
    "\n",
    "for level in range(1, 4):\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(27, 4), sharex=False, sharey=False)\n",
    "    # Count labels for current level\n",
    "    all_locs = sum(hpa_uniprot_combined_trainset[f\"level{level}\"].str.split(\";\").to_list(), [])\n",
    "    counter = Counter(all_locs)\n",
    "    labels = list(counter.keys())\n",
    "    counts = list(counter.values())\n",
    "\n",
    "    # Initialize an empty list to store custom legend handles and labels\n",
    "    handles_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for model in models:\n",
    "        temp = perclass_metrics[(perclass_metrics.category_level == f\"level{level}\") & \n",
    "                            (perclass_metrics.model == model)].set_index(\"label\").reindex(labels)\n",
    "        temp[\"counts\"] = counts\n",
    "        temp[\"model\"] = model\n",
    "        temp[\"level\"] = f\"Level {level}\"\n",
    "\n",
    "        r2_values= []\n",
    "        for i, metric in enumerate(metrics):\n",
    "            sns.scatterplot(\n",
    "                data=temp, \n",
    "                x=\"counts\", \n",
    "                y=metric, \n",
    "                color=model_colors[model],\n",
    "                ax=axes[i],\n",
    "                legend=False  # Disable legend for scatter plot\n",
    "            )\n",
    "            \n",
    "            sns.regplot(\n",
    "                data=temp,\n",
    "                x=\"counts\",\n",
    "                y=metric,\n",
    "                scatter=False,\n",
    "                logx=True,\n",
    "                line_kws={'color': model_colors[model], 'lw': 1.5},\n",
    "                ax=axes[i],\n",
    "            )\n",
    "            axes[i].set_xscale(\"log\")  # <-- Set x-axis to log scale\n",
    "\n",
    "            x = np.log10(temp[\"counts\"].to_numpy())\n",
    "            y = temp[metric].to_numpy()\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "            r_squared = r_value**2\n",
    "            r2_values.append(r_squared)\n",
    "\n",
    "            # Collect legend handles and labels manually for the last plot\n",
    "            if i == len(metrics) - 1:  # Only add the legend to the last plot\n",
    "                handle = Line2D([0], [0], marker='o', color='w', label=model,\n",
    "                                markerfacecolor=model_colors[model], markersize=8)\n",
    "                handles_list.append(handle)\n",
    "                labels_list.append(model)\n",
    "\n",
    "        r2_values_df.append([model, level] + r2_values)\n",
    "    fig.suptitle(f\"Level {level}\", fontsize=16, y=1.02)  # Add figure-wide title indicating the level\n",
    "\n",
    "    # Title and layout\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        ax.set_title(metric_title[metric])\n",
    "        ax.set_xlabel(\"Count (log-scale)\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "\n",
    "    # Create a custom legend outside the figure\n",
    "    fig.legend(\n",
    "        handles_list, labels_list, title=\"Model\",\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.04, 0.85)\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"{FIG_DIR}/custom_CountsvsMetrics_level{level}.pdf\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "r2_values_df = pd.DataFrame(r2_values_df, columns = [\"model\", \"level\"] + metrics)\n",
    "r2_values_df = r2_values_df.set_index(\"model\").round(3)\n",
    "display(r2_values_df)\n",
    "r2_values_df.to_csv(f\"{FIG_DIR}/r2_value_custom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract laprott5 results\n",
    "laprott5_avg_metrics = avg_metrics[(avg_metrics.metadata_file==\"hpa_uniprot_combined_trainset\") &\n",
    "                                    (avg_metrics.clip_len==1024) &\n",
    "                                    (avg_metrics.exp_name==\"ProtT5\") & \n",
    "                                    (avg_metrics.agg_method==\"LightAttentionPool\") &\n",
    "                                    (avg_metrics.loss==\"BCEWithLogitsLoss\") &\n",
    "                                    (avg_metrics.mlp_dropout == 0.25)]\n",
    "laprott5_avg_metrics = laprott5_avg_metrics.drop(\"acc\", axis=1).rename({\"acc_samples\": \"acc\"}, axis=1)\n",
    "\n",
    "run_dir = f\"{SWEEP_EXP_DIR}/ProtT5_hpa_uniprot_combined_trainset\"\n",
    "perclass_metrics =[]\n",
    "for runid in laprott5_avg_metrics.sort_values(\"category_level\").run_id.to_list():\n",
    "    df = pd.read_csv(f\"{run_dir}/{runid}/all_folds_perclass_metrics.csv\")\n",
    "    perclass_metrics.append(df)\n",
    "\n",
    "laprott5_avg_metrics.to_csv(\"../../Benchmark-Models/LAProtT5/LAProtT5_avg_metrics.csv\", index=None)\n",
    "for i, df, in enumerate(perclass_metrics):\n",
    "    df.to_csv(f\"../../Benchmark-Models/LAProtT5/LAProtT5_perclass_metrics_level{i+1}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model to use for downstream analysis\n",
    "idx = avg_metrics[avg_metrics.category_level==\"level1\"][\"macro_ap\"].idxmax()\n",
    "avg_metrics.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(d, run_id, name):\n",
    "\n",
    "    rare_classes = [ #classes with <50 samples in HOU\n",
    "        \"actin-filaments\",\n",
    "        \"intermediate-filaments\", \n",
    "        \"microtubules\",\n",
    "        \"peroxisomes\", \n",
    "        \"lipid-droplets\", \n",
    "        \"nuclear-membrane\", \n",
    "        \"nuclear-speckles\", \n",
    "        \"nucleoli-fibrillar-center\", \n",
    "        \"plastid\"\n",
    "    ]\n",
    "    print(name)\n",
    "    thresholds = np.load(f\"{d}/{run_id}/all_thresholds.npy\")\n",
    "    preds_bin = []\n",
    "    preds_all = []\n",
    "    for i in range(5):\n",
    "        path = f\"{d}/{run_id}/fold_{i}/fold_{i}_test_predictions.csv\"\n",
    "        preds_df = pd.read_csv(path)\n",
    "        cols = preds_df.columns\n",
    "        true_cols = [c for c in cols if \"true\" in c]\n",
    "        pred_cols = [c for c in cols if \"pred\" in c]\n",
    "        if i == 0:\n",
    "            locations = np.array([c.split(\"_\")[0] for c in true_cols])\n",
    "            targets = np.array(preds_df[true_cols].to_numpy())\n",
    "        else:\n",
    "            assert (targets == preds_df[true_cols].to_numpy()).all()\n",
    "            assert (locations == np.array([c.split(\"_\")[0] for c in true_cols])).all()\n",
    "            assert (locations == np.array([c.split(\"_\")[0] for c in pred_cols])).all()\n",
    "        preds = preds_df[pred_cols].to_numpy()\n",
    "        preds = torch.sigmoid(torch.from_numpy(preds)).numpy()\n",
    "        preds_all.append(preds)\n",
    "        preds_bin.append((preds > thresholds[i]).astype(np.int16))\n",
    "    preds_all = np.array(preds_all).mean(axis=0)\n",
    "    preds_bin = (np.stack(preds_bin).mean(axis=0) > 0.5).astype(np.int16)\n",
    "\n",
    "\n",
    "    #Make Precision Recall Line Plot:\n",
    "    palette = sns.color_palette(\"tab20\", n_colors=13)\n",
    "    colors = {\n",
    "        'cytoskeleton': palette[0], \n",
    "        'centrosome': palette[1],\n",
    "        'plasma-membrane': palette[2], \n",
    "        'cytosol': palette[3],\n",
    "        'endoplasmic-reticulum': palette[4], \n",
    "        'endomembrane-system': palette[4],\n",
    "        'golgi-apparatus': palette[5], \n",
    "        'vesicles': palette[6],\n",
    "        'endosomes': palette[7], \n",
    "        'lysosomes': palette[8], \n",
    "        'mitochondria': palette[9], \n",
    "        'nucleoplasm': palette[10], \n",
    "        'nucleus': palette[10],\n",
    "        'nuclear-bodies': palette[11],\n",
    "        'nucleoli': palette[12]\n",
    "    }\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))  \n",
    "    for i in range(targets.shape[1]):\n",
    "        if locations[i] not in rare_classes:\n",
    "            precision, recall, thresholds = precision_recall_curve(targets[:,i], preds_all[:,i])\n",
    "            # Plot the precision-recall curve using Seaborn\n",
    "            sns.lineplot(\n",
    "                x=recall[:-1], \n",
    "                y=precision[:-1], \n",
    "                label=locations[i], \n",
    "                color=colors[locations[i]],\n",
    "                errorbar=None\n",
    "            )\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # Move legend outside the plot (right side)\n",
    "    ax.legend(\n",
    "        title=\"Class\",\n",
    "        bbox_to_anchor=(1.02, 1),  # right outside\n",
    "        loc='upper left',\n",
    "        borderaxespad=0.\n",
    "    )\n",
    "\n",
    "    # Adjust layout so that legend doesn't overlap plot\n",
    "    fig.tight_layout(rect=[0, 0, 0.85, 1])  # Leave space on right for legend\n",
    "\n",
    "    plt.savefig(f\"{FIG_DIR}/{name}_pr_curve.pdf\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    uni_ids = preds_df.id.to_list()\n",
    "    true_locs = [set(locations[row==1]) for row in targets]\n",
    "    pred_locs = [set(locations[row==1]) for row in preds_bin]\n",
    "    df = pd.DataFrame(np.array([uni_ids,true_locs,pred_locs]).T, columns=[\"id\", \"true\", \"pred\"])\n",
    "\n",
    "    def zip_zip(l1,l2):\n",
    "        if len(l1) == 0 and len(l2) > 0:\n",
    "            l = [(\"-\", y) for y in l2]\n",
    "        elif len(l1) > 0 and len(l2) == 0:  \n",
    "            l = [(x, \"-\") for x in l1]\n",
    "        elif len(l1) > 0 and len(l2) > 0:\n",
    "            l = [(x,y) for x in l1 for y in l2]\n",
    "        else: \n",
    "            l = []\n",
    "        return l\n",
    "\n",
    "    df.loc[:, \"same\"] = df.apply(lambda x: x.true.intersection(x.pred), axis=1)\n",
    "    df.loc[:, \"true_not_pred\"] = df.true - df.pred\n",
    "    df.loc[:, \"pred_not_true\"] = df.pred - df.true\n",
    "\n",
    "    df.loc[:, \"replace\"] = df.apply(\n",
    "            lambda x: zip_zip(\n",
    "                x[\"true_not_pred\"], \n",
    "                x[\"pred_not_true\"]), \n",
    "                axis=1)\n",
    "\n",
    "    locations = list(locations) + [\"-\"]\n",
    "    replacement_counter = np.zeros((len(locations), len(locations)))\n",
    "    for row in df[f\"replace\"].to_list():\n",
    "        for replacement in row:\n",
    "            true_loc, pred_loc = replacement\n",
    "            j = locations.index(true_loc)\n",
    "            k = locations.index(pred_loc)\n",
    "            replacement_counter[j,k] += 1\n",
    "\n",
    "    for row in df[f\"same\"].to_list():\n",
    "        for loc in row:\n",
    "            j = locations.index(loc)\n",
    "            replacement_counter[j,j] += 1\n",
    "\n",
    "    true_counts = targets.sum(axis=0)[:, None]\n",
    "    true_counts[locations.index(\"plastid\"), :] = 1\n",
    "    true_replacement_counter = replacement_counter[:-1, :]/true_counts\n",
    "    pred_counts = preds_bin.sum(axis=0)[None, :]\n",
    "    pred_counts[:, locations.index(\"plastid\")] = 1\n",
    "    pred_replacement_counter = replacement_counter[:, :-1]/pred_counts\n",
    "\n",
    "    print(true_replacement_counter.shape)\n",
    "    #There are no true plastids, so delete this row\n",
    "    true_replacement_counter = np.delete(true_replacement_counter, locations.index(\"plastid\"), axis=0)\n",
    "    pred_replacement_counter = np.delete(pred_replacement_counter, locations.index(\"plastid\"), axis=0)\n",
    "    print(true_replacement_counter.shape)\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        1, 2, figsize=(16, 8), sharey=True, gridspec_kw={'width_ratios': [1, 0.93]})\n",
    "    cbar_ax = fig.add_axes([.91, .5, .03, .4])  # Position for the color bar\n",
    "\n",
    "    sns.heatmap(\n",
    "        true_replacement_counter, \n",
    "        xticklabels=locations, \n",
    "        yticklabels=locations[:-2] + [\"-\"], \n",
    "        ax=ax1, \n",
    "        cmap=\"coolwarm\", \n",
    "        cbar_ax=cbar_ax\n",
    "        )\n",
    "    ax1.set_xlabel(\"Predicted label\")\n",
    "    ax1.set_ylabel(\"True label\")\n",
    "    ax1.set_title(\"True Confusion Matrix\", fontsize=16)\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=60, ha='right')\n",
    "\n",
    "    sns.heatmap(\n",
    "        pred_replacement_counter, \n",
    "        xticklabels=locations[:-1], \n",
    "        yticklabels=locations[:-2] + [\"-\"], \n",
    "        ax=ax2, \n",
    "        cmap=\"coolwarm\", \n",
    "        cbar_ax=cbar_ax,\n",
    "        )\n",
    "    ax2.set_xlabel(\"Predicts label\")\n",
    "    ax1.set_ylabel(\"True label\")\n",
    "    ax2.set_title(\"Predicted Confusion Matrix\", fontsize=16)\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=60, ha='right')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, .9, 1])  # Adjust layout to make space for the color bar\n",
    "    plt.savefig(f\"{FIG_DIR}/{name}.pdf\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProtT5 is best in general, so get best ProtT5 models for each level\n",
    "data_uniprot_combined_trainset = avg_metrics[\n",
    "    (avg_metrics.metadata_file == \"hpa_uniprot_combined_trainset\")\n",
    "]\n",
    "idx = data_uniprot_combined_trainset.groupby(\n",
    "    [\"exp_name\", \"category_level\"]\n",
    ").macro_ap.transform(max) == data_uniprot_combined_trainset['macro_ap']\n",
    "temp = data_uniprot_combined_trainset[idx]\n",
    "temp = temp[temp.exp_name == \"ProtT5\"]\n",
    "\n",
    "d = f\"{SWEEP_EXP_DIR}/ProtT5_hpa_uniprot_combined_trainset\"\n",
    "for i, row in temp.iterrows():\n",
    "    name = f\"{row.exp_name}_{row.category_level}_confusionmatrices\"\n",
    "    run_id = row.run_id\n",
    "    make_matrix(d, run_id, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAProtT5 is best benchmark model in general, so also make confusion matrices for this\n",
    "\n",
    "temp = laprott5_avg_metrics\n",
    "\n",
    "d = f\"{SWEEP_EXP_DIR}/ProtT5_hpa_uniprot_combined_trainset\"\n",
    "for i, row in temp.iterrows():\n",
    "    name = f\"LAProtT5_{row.category_level}_confusionmatrices\"\n",
    "    run_id = row.run_id\n",
    "    make_matrix(d, run_id, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
